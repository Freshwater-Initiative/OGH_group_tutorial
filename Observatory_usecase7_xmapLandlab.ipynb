{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve NetCDF and model gridded climate time-series for a watershed\n",
    "\n",
    "### Case study:  the Sauk-Suiattle Watershed\n",
    "<img src=\"http://www.sauk-suiattle.com/images/Elliott.jpg\" \n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "### Use this Jupyter Notebook to:\n",
    "    1. Prepare computing environment\n",
    "    2. Get list of grid cells\n",
    "    3. NetCDF retrieval and clipping to a spatial extent\n",
    "    4. Extract NetCDF metadata and convert NetCDFs to 1D ASCII time-series files\n",
    "    5. Visualize the average monthly total precipitations\n",
    "    6. Apply summary values as modeling inputs\n",
    "    7. Visualize modeling outputs\n",
    "    8. Save results in a new HydroShare resource\n",
    "\n",
    "##### For inquiries about functions, please refer to https://github.com/freshwater-initiative/Observatory \n",
    "\n",
    "<br/><br/><br/>\n",
    "<img src=\"https://www.washington.edu/brand/files/2014/09/W-Logo_Purple_Hex.png\"\n",
    "style=\"float:right;width:150px;padding:20px\">\n",
    "\n",
    "<br/><br/>\n",
    "### Special thanks to Nicoleta Cristea and Jeff Keck for their contribution to this work\n",
    "#### This data is compiled to digitally observe the watersheds, powered by HydroShare. <br/>Provided by the Watershed Dynamics Group, Dept. of Civil and Environmental Engineering, University of Washington"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Prepare computing environment\n",
    "\n",
    "Here, we import ogh and several requisite libraries. Then, map directories that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# If OGH is not installed, uncomment and run the line below\n",
    "\n",
    "#!conda install -c conda-forge ogh xarray=0.11.2 --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize data analysis libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silencing warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, pandas as pd, numpy as np\n",
    "import ogh\n",
    "from ogh import oxl\n",
    "from ecohydrology_model_functions import run_ecohydrology_model, plot_results\n",
    "from landlab import imshow_grid, CLOSED_BOUNDARY\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a secure connection with HydroShare REST API by initializing the class from the utilities folder. This command also sets environment variables that will be useful for saving work back to HydroShare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utilities import hydroshare\n",
    "hs=hydroshare.hydroshare()\n",
    "homedir = os.getcwd()\n",
    "\n",
    "# map modeling input params\n",
    "InputFile = os.path.join(homedir,'ecohyd_inputs.yaml')\n",
    "\n",
    "# initialize ogh metadata\n",
    "meta_file = dict(ogh.ogh_meta())\n",
    "\n",
    "# initialize list of outputs\n",
    "files=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homedir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get list of grid cells\n",
    "\n",
    "For visualization purposes, we will also remap the study site shapefile, which is stored in HydroShare at the following url: https://www.hydroshare.org/resource/c532e0578e974201a0bc40a37ef2d284/. Since the shapefile was previously migrated, we can select 'N' for no overwriting.\n",
    "\n",
    "In the usecase1 notebook, the treatgeoself function identified the gridded cell centroid coordinates that overlap with our study site. These coordinates were documented within the mapping file, which will be remapped here. In the usecase2 notebook, the downloaded files were cataloged within the mapping file, so we will use the mappingfileSummary function to characterize the files available for Sauk-Suiattle for each gridded data product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1/16-degree Gridded cell centroids\n",
    "\"\"\"\n",
    "# List of available data\n",
    "hs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f')\n",
    "NAmer = hs.content['NAmer_dem_list.shp']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sauk\n",
    "\"\"\"\n",
    "# Watershed extent\n",
    "hs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284')\n",
    "sauk = hs.content['wbdhub12_17110006_WGS84_Basin.shp']\n",
    "\n",
    "# reproject the shapefile into WGS84\n",
    "ogh.reprojShapefile(sourcepath=sauk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the target grid cells within a watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# map the mappingfiles from usecase1\n",
    "mappingfile1=ogh.treatgeoself(shapefile=sauk, NAmer=NAmer, buffer_distance=0.06,\n",
    "                              mappingfile=os.path.join(homedir,'Sauk_mappingfile.csv'))\n",
    "\n",
    "files.append(mappingfile1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  NetCDF retrieval and clipping to a spatial extent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section performs data retrieval and clips the datafile to the bounding box for your watershed. You will source datafiles from the Dr. Ben Livneh's 2013 publication for interpolated meteorological observations.\n",
    "\n",
    "ftp://livnehpublicstorage.colorado.edu/public/Livneh.2013.CONUS.Dataset/Meteorology.nc.v.1.2.1915.2011.bz2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(oxl.get_x_dailymet_Livneh2013_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function get_x_dailymet_livneh2013_raw retrieves and clips NetCDF files archived within the University of Colorado Boulder repository. This archive contains daily data captured for the Continental US, stored in monthly files.\n",
    "\n",
    "We want to consider meteorology for Sauk-Suiattle watershed with data representing 1970. In the code chunk below, 6 parallel workers will be initialized to distribute the tasks. Each worker will be tasked to ftp request a file, clip the netcdf file to the provided spatial bounding box (in WGS84 standard projection), then return the location of the spatially clipped files. That is one task.\n",
    "\n",
    "Provide the home and subdirectory as inputs for where the cropped NetCDF files will be stored, the number of workers to carry out the tasks, and the start and end dates of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "maptable, nstations = ogh.mappingfileToDF(mappingfile1, summary=True)\n",
    "spatialbounds = {'minx':maptable.LONG_.min(), 'maxx':maptable.LONG_.max(),\n",
    "                 'miny':maptable.LAT.min(), 'maxy':maptable.LAT.max()}\n",
    "\n",
    "outputfiles = oxl.get_x_dailymet_Livneh2013_raw(homedir=homedir,\n",
    "                                                subdir='livneh2013/Daily_MET_1970_1970/raw_netcdf',\n",
    "                                                spatialbounds=spatialbounds,\n",
    "                                                nworkers=6,\n",
    "                                                start_date='1970-01-01', end_date='1970-12-31')\n",
    "\n",
    "files.extend(outputfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract NetCDF metadata and convert NetCDFs to 1D ASCII time-series files\n",
    "\n",
    "Here, convert the netCDF files into daily ascii time-series files for each grid cell location. For the 99 gridded data files intended, a 1D time-series ASCII will be created. The NetCDF metadata will be extracted to the meta_file dictionary using the catalog_label as the dictionary key.\n",
    "\n",
    "Provide the home and subdirectory where the ASCII files will be stored, the directory for the source netCDF files, and the mapping file to which the resulting ASCII files will be cataloged. Also, provide the Pandas Datetime code for the frequency of the data time steps. Finally, provide the catalog label that will be used for the mapping file catalog and the metadata file label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "outfilelist = oxl.netcdf_to_ascii(homedir=homedir, \n",
    "                                  subdir='livneh2013/Daily_MET_1970_1970/raw_ascii', \n",
    "                                  source_directory=os.path.join(homedir, 'livneh2013/Daily_MET_1970_1970/raw_netcdf'),\n",
    "                                  mappingfile=mappingfile1,\n",
    "                                  temporal_resolution='D',\n",
    "                                  meta_file=meta_file,\n",
    "                                  catalog_label='sp_dailymet_livneh_1970_1970')\n",
    "\n",
    "# files.extend(outfilelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(meta_file.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize file availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maptable, nstations = ogh.mappingfileToDF(mappingfile1, summary=True)\n",
    "maptable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the acquired metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_file['sp_dailymet_livneh_1970_1970']['variable_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the metadata\n",
    "metafile_path = os.path.join(homedir, 'test.json')\n",
    "ogh.saveDictOfDf(dictionaryObject=meta_file, outfilepath=metafile_path)\n",
    "files.append(metafile_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of climate variables for the long-term mean (ltm).\n",
    "#### INPUT: gridded meteorology ASCII files located from the Sauk-Suiattle Mapping file. The locations are cataloged within the mappingfile, under the dataset catalog label. The gridclim_dict function will read in the files, and automatically compute the basic statistics for each variable.\n",
    "\n",
    "#### OUTPUT: dictionary of dataframes where rows are temporal summaries and columns are spatial summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ltm = ogh.gridclim_dict(mappingfile=mappingfile1,\n",
    "                        metadata=meta_file,\n",
    "                        dataset='sp_dailymet_livneh_1970_1970',\n",
    "                        variable_list=['Prec','Tmax','Tmin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(ltm.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm['meanbymonth_Prec_sp_dailymet_livneh_1970_1970']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the total monthly and yearly precipitation, as well as the mean values across time and across stations. Append into the ltm dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract metadata\n",
    "dr = meta_file['sp_dailymet_livneh_1970_1970']\n",
    "\n",
    "# compute sums and mean monthly an yearly sums\n",
    "ltm = ogh.aggregate_space_time_sum(df_dict=ltm,\n",
    "                                   suffix='Prec_sp_dailymet_livneh_1970_1970',\n",
    "                                   start_date=dr['start_date'],\n",
    "                                   end_date=dr['end_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the name of the analytical dataframes to see the new additions\n",
    "# sorted(ltm.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the destination path for the dictionary of dataframes\n",
    "ltm_sauk=os.path.join(homedir, 'ltm_1970_1970_sauk.json')\n",
    "ogh.saveDictOfDf(dictionaryObject=ltm, outfilepath=ltm_sauk)\n",
    "files.append(ltm_sauk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize the average monthly total precipitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one highest elevation location\n",
    "highE_ref = ogh.findCentroidCode(mappingfile=mappingfile1, colvar='ELEV', colvalue=2216)\n",
    "\n",
    "# two lowest elevation locations\n",
    "lowE_ref = ogh.findCentroidCode(mappingfile=mappingfile1, colvar='ELEV', colvalue=164)\n",
    "\n",
    "# combine references together\n",
    "reference_lines = highE_ref + lowE_ref\n",
    "reference_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the monthly time-series (in the wateryear schedule)\n",
    "\n",
    "Each point in the boxplot distributions is a station's value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# INPUT: dataframe with each month as a row and each station as a column. \n",
    "# OUTPUT: A png file that represents the distribution across stations (in Wateryear order)\n",
    "\n",
    "ogh.renderValueInBoxplot(ltm['meanbymonthsum_Prec_sp_dailymet_livneh_1970_1970'],\n",
    "                         outfilepath=os.path.join(homedir, 'totalMonthlyRainfall.png'), \n",
    "                         plottitle='Total monthly rainfall',\n",
    "                         time_steps='month',\n",
    "                         wateryear=True,\n",
    "                         reference_lines=reference_lines,\n",
    "                         ref_legend=True,\n",
    "                         obs_legend=False,\n",
    "                         value_name='Total daily precipitation (mm)',\n",
    "                         cmap='seismic_r',\n",
    "                         figsize=(6,6))\n",
    "\n",
    "files.append(os.path.join(homedir, 'totalMonthlyRainfall.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the spatial distribution for a select month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ogh.renderValuesInPoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# INPUT: dataframe with each month as a row and each station as a column. \n",
    "# OUTPUT: A png file that represents the December spatial distribution of total precipitation\n",
    "\n",
    "ogh.renderValuesInPoints(ltm['meanbymonthsum_Prec_sp_dailymet_livneh_1970_1970'], \n",
    "                         vardf_dateindex=12, \n",
    "                         shapefile=sauk, \n",
    "                         cmap='seismic_r',\n",
    "                         gridcell_alpha=1,\n",
    "                         outfilepath='test.png', \n",
    "                         plottitle='December total rainfall',\n",
    "                         colorbar_label='Total monthly rainfall (mm)', \n",
    "                         figsize=(1.5,1.5))\n",
    "\n",
    "files.append(os.path.join(homedir, 'test.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Apply summary values as modeling inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the spatial bounding box to raster dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# compute the dimensions of the raster\n",
    "minx2, miny2, maxx2, maxy2 = oxl.calculateUTMbounds(mappingfile=mappingfile1,\n",
    "                                                    mappingfile_crs={'init':'epsg:4326'},\n",
    "                                                    spatial_resolution=0.06250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(minx2, miny2, maxx2, maxy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the raster\n",
    "\n",
    "The Ecohydrology vegetation model within Landlab is run for a 2D landscape. The data input should be a raster array object configured for the amount of grid cell dimensions desired. Here, the raster dimensions are used to configure 1kmx1km grid cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# generate a raster with 1kmx1km grid cells\n",
    "raster, row_list, col_list = oxl.rasterDimensions(minx=minx2, miny=miny2, maxx=maxx2, maxy=maxy2, dx=1000, dy=1000)\n",
    "\n",
    "# print the raster dimensions (column and rows)\n",
    "raster.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher resolution children of gridded cells \n",
    "\n",
    "Relate the parent grid cells (in the mappingfile) to higher resolution children nodes in the raster. Maintain this connection in the nodeXmap object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(oxl.mappingfileToRaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# landlab raster node crossmap to gridded cell id\n",
    "nodeXmap, raster, m = oxl.mappingfileToRaster(mappingfile=mappingfile1, \n",
    "                                              spatial_resolution=0.06250,\n",
    "                                              minx=minx2, miny=miny2, maxx=maxx2, maxy=maxy2,\n",
    "                                              dx=1000, dy=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeXmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the elevation within the high resolutions children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nodeXmap.fillna(nodeXmap['ELEV'].min()).plot(column='ELEV', figsize=(10,10), legend=True, cmap='terrain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the average monthly precipitation for December relative to all months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# generate vector array of December monthly precipitation\n",
    "prec_vector = oxl.rasterVector(vardf=ltm['meanbymonthsum_Prec_sp_dailymet_livneh_1970_1970'],\n",
    "                               vardf_dateindex=12,\n",
    "                               crossmap=nodeXmap,\n",
    "                               nodata=-9999)\n",
    "\n",
    "# close-off areas without data\n",
    "raster.status_at_node[prec_vector==-9999] = CLOSED_BOUNDARY\n",
    "\n",
    "fig =plt.figure(figsize=(10,10))\n",
    "imshow_grid(raster,\n",
    "            prec_vector,\n",
    "            var_name='Monthly precipitation',\n",
    "            var_units=meta_file['sp_dailymet_livneh_1970_1970']['variable_info']['Prec'].attrs['units'],\n",
    "            color_for_closed='black',\n",
    "            cmap='seismic_r',\n",
    "            vmin=ltm['meanbymonthsum_Prec_sp_dailymet_livneh_1970_1970'].unstack().min(),\n",
    "            vmax=ltm['meanbymonthsum_Prec_sp_dailymet_livneh_1970_1970'].unstack().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize average maximum daily temperature for December relative to all months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_vector = oxl.rasterVector(vardf=ltm['meanbymonth_Tmax_sp_dailymet_livneh_1970_1970'],\n",
    "                               vardf_dateindex=12,\n",
    "                               crossmap=nodeXmap,\n",
    "                               nodata=-9999)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "imshow_grid(raster,\n",
    "            tmax_vector,\n",
    "            var_name='Daily maximum temperature',\n",
    "            var_units=meta_file['sp_dailymet_livneh_1970_1970']['variable_info']['Tmax'].attrs['units'],\n",
    "            color_for_closed='black',\n",
    "            cmap='magma',\n",
    "            vmax=ltm['meanbymonth_Tmax_sp_dailymet_livneh_1970_1970'].unstack().max(),\n",
    "            vmin=ltm['meanbymonth_Tmax_sp_dailymet_livneh_1970_1970'].unstack().min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the average minimum daily temperature for December relative to all months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin_vector = oxl.rasterVector(vardf=ltm['meanbymonth_Tmin_sp_dailymet_livneh_1970_1970'],\n",
    "                               vardf_dateindex=12,\n",
    "                               crossmap=nodeXmap,\n",
    "                               nodata=-9999)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "imshow_grid(raster, \n",
    "            tmin_vector,\n",
    "            var_name='Daily minimum temperature',\n",
    "            var_units=meta_file['sp_dailymet_livneh_1970_1970']['variable_info']['Tmin'].attrs['units'],\n",
    "            color_for_closed='black', \n",
    "            cmap='magma',\n",
    "            vmax=ltm['meanbymonth_Tmin_sp_dailymet_livneh_1970_1970'].unstack().max(),\n",
    "            vmin=ltm['meanbymonth_Tmin_sp_dailymet_livneh_1970_1970'].unstack().min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the average monthly precipitation for December, again, but using Google standard projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a raster vector back to geospatial presentation\n",
    "t2, t3 = oxl.rasterVectorToWGS(prec_vector, nodeXmap=nodeXmap, UTM_transformer=m)\n",
    "\n",
    "t2.crs = {'init':'epsg:4326'}\n",
    "t2.drop('raster_geom', axis=1).to_file(os.path.join(homedir,'hires_sauk.shp'))\n",
    "t2.plot(column='value', figsize=(10,6), legend=True, cmap='seismic_r',\n",
    "        vmin=ltm['meanbymonthsum_Prec_sp_dailymet_livneh_1970_1970'].unstack().min(), \n",
    "        vmax=ltm['meanbymonthsum_Prec_sp_dailymet_livneh_1970_1970'].unstack().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the average monthly precipitation for December, again, but using the North Washington projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t2 = t2.to_crs({'init':'epsg:3857'})\n",
    "t2.plot(column='value', figsize=(10,6), legend=True, cmap='seismic_r',\n",
    "        vmin=ltm['meanbymonthsum_Prec_sp_dailymet_livneh_1970_1970'].unstack().min(), \n",
    "        vmax=ltm['meanbymonthsum_Prec_sp_dailymet_livneh_1970_1970'].unstack().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in the shp file parts\n",
    "files.extend([os.path.join(homedir, shpfile) for shpfile in os.listdir(homedir) if 'hires_sauk' in shpfile])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the meteorological summary for 1970 as the model input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the input vector with 15000 repetitions \n",
    "inputvectors = {'precip_met': np.tile(ltm['meandaily_Prec_sp_dailymet_livneh_1970_1970'], 15000),\n",
    "                'Tmax_met': np.tile(ltm['meandaily_Tmax_sp_dailymet_livneh_1970_1970'], 15000),\n",
    "                'Tmin_met': np.tile(ltm['meandaily_Tmin_sp_dailymet_livneh_1970_1970'], 15000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# run ecohydrology model for 100000 storms\n",
    "\n",
    "(VegType_low, yrs_low, debug_low) = run_ecohydrology_model(raster,\n",
    "                                                           input_data=inputvectors,\n",
    "                                                           input_file=InputFile,\n",
    "                                                           synthetic_storms=False,\n",
    "                                                           number_of_storms=100000,\n",
    "                                                           pet_method='PriestleyTaylor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize modeling outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_results(raster, VegType_low, yrs_low, yr_step=yrs_low-1)\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(homedir,'grid_low.png'))\n",
    "files.append(os.path.join(homedir,'grid_low.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the results back into HydroShare\n",
    "\n",
    "Create a new hydroshare resource to store the Geoprocessing output files listed within 'files'. First, define all of the required metadata for resource creation, i.e. *title*, *abstract*, *keywords*, *content files*. In addition, we must define the type of resource that will be created, in this case *compositeresource*.  \n",
    "\n",
    "***Note:*** Make sure you save the notebook at this point, so that all notebook changes will be saved into the new HydroShare resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add the notebook\n",
    "files.append(os.path.join(homedir, 'Observatory_usecase7_xmapLandlab.ipynb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total files and image to migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file downloaded onto the server folder, move to a new HydroShare Composite Resource\n",
    "title = 'Computed spatial-temporal summaries for Sauk-Suiattle for 1970'\n",
    "abstract = 'This resource contains the computed summaries for the Meteorology data from Livneh et al. 2013.'\n",
    "keywords = ['Sauk-Suiattle', 'Livneh 2013', 'climate', 'hydromet', 'watershed', 'visualizations and summaries'] \n",
    "rtype = 'compositeresource'\n",
    "parent_resource_id = homedir.replace('/data/contents','').rsplit('/',1)[1]\n",
    "\n",
    "# create the new resource\n",
    "resource_id = hs.createHydroShareResource(abstract,\n",
    "                                          title,\n",
    "                                          keywords=keywords,\n",
    "                                          resource_type=rtype,\n",
    "                                          content_files=files,\n",
    "                                          derivedFromId=parent_resource_id,\n",
    "                                          public=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for trying this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # ### Once done, uncomment to empty the stored data ###\n",
    "# # # \n",
    "# # #\n",
    "# # # # Delete items in 'files' but skip this notebook\n",
    "# # # for eachf in files[:-1]:\n",
    "# # #     os.remove(eachf)\n",
    "# # # \n",
    "# # # \n",
    "# # # import shutil\n",
    "# # # shutil.rmtree(os.path.join(homedir, 'livneh2013'))\n",
    "# # # \n",
    "# # # "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
